import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
import warnings
import os
import pymysql
from config import get_db_config
from contextlib import contextmanager
warnings.filterwarnings('ignore')


@contextmanager
def get_mysql_connection():
	"""MySQL ì—°ê²° ìƒì„± (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)"""
	conn = pymysql.connect(**get_db_config())
	try:
		yield conn
	finally:
		conn.close()


class SalesBasedFiltering:
	def __init__(self, movies_file='ml-latest-small/movies.csv'):
		"""
		Sales ê¸°ë°˜ ì½˜í…ì¸  í•„í„°ë§ í´ë˜ìŠ¤ (ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ ì—°ë™)

		Args:
			movies_file: ì˜í™” ì •ë³´ íŒŒì¼ ê²½ë¡œ
		"""
		self.movies_file = movies_file
		self.ratings_df = None
		self.movies_df = None

		self.sale_index_to_id = None  # list[int saleId] aligned to row index
		self.sale_index_to_movie_id = None  # list[int movieId] aligned to row index
		self.sale_index_by_movie_id = None  # dict[int movieId -> list[int sale_row_index]]
		
		self.token_to_index = None
		self.index_to_token = None
		self.idf_vector = None
		self.item_feature_matrix = None  # csr_matrix (num_sales x num_features)
		self.sales_meta_by_id = None  # dict[int saleId -> dict row]

	def load_data(self):
		"""ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í‰ì  ë°ì´í„° ë° ì˜í™” ë°ì´í„° ë¡œë“œ"""
		print("ë°ì´í„°ë² ì´ìŠ¤ reviews í…Œì´ë¸”ì—ì„œ í‰ì  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
		self.ratings_df = self._load_ratings_from_database()
		
		if self.ratings_df is not None and not self.ratings_df.empty:
			print(f"í‰ì  ë°ì´í„°: {self.ratings_df.shape[0]}ê°œ í‰ì ")
			print(f"ì‚¬ìš©ì ìˆ˜: {self.ratings_df['userId'].nunique()}ëª…")
			print(f"ì˜í™” ìˆ˜: {self.ratings_df['movieId'].nunique()}ê°œ")
		else:
			print("âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í‰ì  ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
			raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ reviews í…Œì´ë¸”ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
		
		print("ì˜í™” ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
		if os.path.exists(self.movies_file):
			self.movies_df = pd.read_csv(self.movies_file)
			print(f"ì˜í™” ë°ì´í„°: {self.movies_df.shape[0]}ê°œ ì˜í™”")
		else:
			print(f"ì˜í™” ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.movies_file}")
			self.movies_df = None

	def _load_ratings_from_database(self):
		"""ë°ì´í„°ë² ì´ìŠ¤ reviews í…Œì´ë¸”ì—ì„œ í‰ì  ë°ì´í„° ë¡œë“œ"""
		try:
			with get_mysql_connection() as conn:
				query = """
				SELECT 
					r.user_id as userId,
					s.movie_id as movieId,
					r.rating as rating,
					UNIX_TIMESTAMP(r.created_at) as timestamp
				FROM reviews r
				LEFT JOIN sales s ON r.sales_id = s.id
				WHERE s.movie_id IS NOT NULL
				ORDER BY r.user_id, r.sales_id
				"""
				
				ratings_df = pd.read_sql(query, con=conn)
				
				if ratings_df.empty:
					print("âš ï¸ Reviews í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
					return None
				
				# ë°ì´í„° íƒ€ì… í™•ì¸ ë° ì •ë¦¬
				ratings_df['userId'] = ratings_df['userId'].astype(int)
				ratings_df['movieId'] = ratings_df['movieId'].astype(int)
				ratings_df['rating'] = ratings_df['rating'].astype(float)
				ratings_df['timestamp'] = ratings_df['timestamp'].fillna(0).astype(int)
				
				print(f"âœ… Reviews í…Œì´ë¸”ì—ì„œ {len(ratings_df)}ê°œ í‰ì  ë¡œë“œ ì™„ë£Œ")
				return ratings_df
				
		except Exception as e:
			print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ í‰ì  ë¡œë“œ ì˜¤ë¥˜: {e}")
			return None

	def _price_bucket(self, price):
		"""ê°€ê²©ì„ ë§Œì› ë‹¨ìœ„ ë²„í‚·ìœ¼ë¡œ ë³€í™˜"""
		if pd.isna(price):
			return None

		try:
			p = int(price)
		except Exception:
			return None

		if p < 10000:
			return 'price:<1ë§Œ'
		band = p // 10000
		return f'price:{band}ë§ŒëŒ€'

	def create_sales_feature_matrix(self, min_df=2, use_log_tf=True, engine='sklearn'):
		"""
		Sales ë©”íƒ€ë°ì´í„° ê¸°ë°˜ TF-IDF íŠ¹ì„± í–‰ë ¬ ìƒì„± (quality/price/region/site/limited)

		Args:
			min_df: í† í° ìµœì†Œ ë¬¸ì„œ ë¹ˆë„
			use_log_tf: ë¡œê·¸ TF ì‚¬ìš© ì—¬ë¶€
			engine: 'sklearn' ë˜ëŠ” 'internal' (ê¸°ë³¸ 'sklearn')
		"""
		print("MySQLì—ì„œ sales ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")

		query = (
			"SELECT id, movie_id, site_name, price, quality, region_code, is_limited_edition, "
			"bluray_title, site_url "
			"FROM sales"
		)
		with get_mysql_connection() as conn:
			df = pd.read_sql(query, con=conn)
		if df.empty:
			raise RuntimeError("sales í…Œì´ë¸”ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

		# ì¸ë±ìŠ¤/ë§¤í•‘ ì´ˆê¸°í™”
		sale_ids = df['id'].astype(int).tolist()
		self.sale_index_to_id = sale_ids

		# movie_id -> sale row indices ë§¤í•‘ êµ¬ì„±
		self.sale_index_by_movie_id = {}
		self.sale_index_to_movie_id = []
		for idx, row in df.iterrows():
			mid = int(row['movie_id']) if not pd.isna(row['movie_id']) else None
			self.sale_index_to_movie_id.append(mid)
			if mid is not None:
				self.sale_index_by_movie_id.setdefault(mid, []).append(idx)
		

		# ê³µí†µ: í† í° í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜
		def _row_to_token_text(row):
			tokens = []
			q = row.get('quality')
			if pd.notna(q) and str(q).strip():
				tokens.append(f"quality:{str(q).strip()}")
			r = row.get('region_code')
			if pd.notna(r) and str(r).strip():
				tokens.append(f"region:{str(r).strip()}")
			site = row.get('site_name')
			if pd.notna(site) and str(site).strip():
				tokens.append(f"site:{str(site).strip().lower()}")
			limited = row.get('is_limited_edition')
			if pd.notna(limited):
				tokens.append(f"limited:{'yes' if bool(limited) else 'no'}")
			bucket = self._price_bucket(row.get('price'))
			if bucket:
				tokens.append(bucket)
			return " ".join(tokens)

		if engine == 'sklearn':
			try:
				from sklearn.feature_extraction.text import TfidfVectorizer
			except Exception as e:
				print(f"scikit-learn ì‚¬ìš© ì‹¤íŒ¨({e}), internal ì—”ì§„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
				engine = 'internal'
		
		if engine == 'sklearn':
			texts = [
				_row_to_token_text(row)
				for _, row in df.iterrows()
			]
			vectorizer = TfidfVectorizer(
				lowercase=False,
				preprocessor=lambda s: s,
				tokenizer=str.split,
				token_pattern=None,
				min_df=min_df,
				sublinear_tf=bool(use_log_tf),
				norm='l2',
				dtype=np.float32,
			)
			matrix = vectorizer.fit_transform(texts)
			self.item_feature_matrix = matrix.tocsr()
			# í˜¸í™˜ì„±ì„ ìœ„í•´ ë³´ì¡° ë©”íƒ€ë°ì´í„°ë„ ìœ ì§€
			self.token_to_index = dict(vectorizer.vocabulary_)
			self.index_to_token = {idx: tok for tok, idx in self.token_to_index.items()}
			# idf_ëŠ” feature ìˆœì„œ ê¸°ì¤€
			try:
				idf = np.asarray(vectorizer.idf_, dtype=np.float32)
				self.idf_vector = idf
			except Exception:
				self.idf_vector = None
			# íŒë§¤ ë©”íƒ€ë°ì´í„°ë¥¼ id ê¸°ë°˜ìœ¼ë¡œ ì¸ë©”ëª¨ë¦¬ ì¸ë±ì‹±
			self.sales_meta_by_id = {int(r['id']): dict(r) for _, r in df.iterrows()}
			print(f"íŠ¹ì„± í–‰ë ¬ í¬ê¸°: {self.item_feature_matrix.shape} (sales x í† í°, engine=sklearn)")
			return

		# ë‚´ë¶€ ì—”ì§„ ê²½ë¡œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
		item_tokens_list = []
		df_counter = {}
		for _, row in df.iterrows():
			tokens = {}
			q = row.get('quality')
			if pd.notna(q) and str(q).strip():
				key = f"quality:{str(q).strip()}"
				tokens[key] = tokens.get(key, 0) + 1
			r = row.get('region_code')
			if pd.notna(r) and str(r).strip():
				key = f"region:{str(r).strip()}"
				tokens[key] = tokens.get(key, 0) + 1
			site = row.get('site_name')
			if pd.notna(site) and str(site).strip():
				key = f"site:{str(site).strip().lower()}"
				tokens[key] = tokens.get(key, 0) + 1
			limited = row.get('is_limited_edition')
			if pd.notna(limited):
				key = f"limited:{'yes' if bool(limited) else 'no'}"
				tokens[key] = tokens.get(key, 0) + 1
			bucket = self._price_bucket(row.get('price'))
			if bucket:
				tokens[bucket] = tokens.get(bucket, 0) + 1

			item_tokens_list.append(tokens)
			for tok in tokens.keys():
				df_counter[tok] = df_counter.get(tok, 0) + 1

		num_items = len(sale_ids)
		valid_tokens = [tok for tok, dfv in df_counter.items() if dfv >= min_df]
		valid_tokens.sort()
		self.token_to_index = {tok: idx for idx, tok in enumerate(valid_tokens)}
		self.index_to_token = {idx: tok for tok, idx in self.token_to_index.items()}

		idf = np.zeros(len(self.token_to_index), dtype=np.float32)
		for tok, idx in self.token_to_index.items():
			df_v = df_counter.get(tok, 0)
			idf[idx] = np.log((1.0 + num_items) / (1.0 + df_v)) + 1.0
		self.idf_vector = idf

		rows, cols, data = [], [], []
		for row_idx, token_counts in enumerate(item_tokens_list):
			for tok, tf in token_counts.items():
				col_idx = self.token_to_index.get(tok)
				if col_idx is None:
					continue
				val_tf = (1.0 + np.log(tf)) if use_log_tf else float(tf)
				rows.append(row_idx)
				cols.append(col_idx)
				data.append(val_tf)

		matrix = csr_matrix((np.array(data, dtype=np.float32), (np.array(rows), np.array(cols))),
			shape=(num_items, len(self.token_to_index)), dtype=np.float32)
		matrix = matrix.multiply(self.idf_vector)
		row_norms = np.asarray(np.sqrt(matrix.multiply(matrix).sum(axis=1))).ravel()
		row_norms[row_norms == 0.0] = 1.0
		inv = 1.0 / row_norms
		matrix = matrix.multiply(inv.reshape(-1, 1))

		self.item_feature_matrix = matrix.tocsr()
		# íŒë§¤ ë©”íƒ€ë°ì´í„°ë¥¼ id ê¸°ë°˜ìœ¼ë¡œ ì¸ë©”ëª¨ë¦¬ ì¸ë±ì‹±
		self.sales_meta_by_id = {int(r['id']): dict(r) for _, r in df.iterrows()}
		print(f"íŠ¹ì„± í–‰ë ¬ í¬ê¸°: {self.item_feature_matrix.shape} (sales x í† í°, engine=internal)")

	def _build_user_profile_vector(self, user_id):
		"""ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„° ìƒì„± (í‰ì  ê°€ì¤‘ í•©)"""
		if user_id not in set(self.ratings_df['userId'].unique().tolist()):
			return None

		user_ratings = self.ratings_df[self.ratings_df['userId'] == user_id][['movieId', 'rating']]
		user_ratings['movieId'] = user_ratings['movieId'].astype(int)

		# ì‚¬ìš©ì í‰ê·  ì¤‘ì‹¬í™” ê°€ì¤‘ì¹˜ (ëª¨ë“  í‰ì ì´ ê°™ìœ¼ë©´ í‰ì ê°’ ì‚¬ìš©)
		user_mean = user_ratings['rating'].mean()
		user_ratings['weight'] = user_ratings['rating'] - user_mean
		
		# ëª¨ë“  í‰ì ì´ ê°™ì•„ì„œ ê°€ì¤‘ì¹˜ê°€ 0ì´ ë˜ëŠ” ê²½ìš°, í‰ì  ìì²´ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
		if (user_ratings['weight'] == 0).all():
			user_ratings['weight'] = user_ratings['rating']

		# ì˜í™” ì¸ë±ìŠ¤ ì •ë ¬ ë° ê°€ì¤‘ì¹˜ ë²¡í„° ìƒì„±
		indices = []
		weights = []
		for _, r in user_ratings.iterrows():
			mid = int(r['movieId'])
			w = float(r['weight'])
			if w == 0.0:
				continue
			
			# ì˜í™”ì— ì—°ê²°ëœ ëª¨ë“  sales í–‰ì— ê°€ì¤‘ì¹˜ë¥¼ ê· ë“± ë¶„ë°°
			row_indices = self.sale_index_by_movie_id.get(mid, [])
			if not row_indices:
				continue
			share = w / float(len(row_indices))
			for idx in row_indices:
				indices.append(int(idx))
				weights.append(share)

		if not indices:
			return None

		# ê°€ì¤‘ í•©: w^T * M  (1 x F)
		w_vec = np.zeros(self.item_feature_matrix.shape[0], dtype=np.float32)
		w_vec[np.array(indices, dtype=int)] = np.array(weights, dtype=np.float32)
		profile = w_vec @ self.item_feature_matrix  # dense(1 x F)

		# L2 ì •ê·œí™”
		norm = np.linalg.norm(profile)
		if norm == 0.0:
			return None
		return (profile / norm).astype(np.float32)

	def recommend_sales(self, user_id, n_recommendations=10, min_score=0.0):
		"""
		íŠ¹ì • ì‚¬ìš©ìì—ê²Œ sales ì¶”ì²œ (ì½˜í…ì¸  ê¸°ë°˜)

		Args:
			user_id: ì‚¬ìš©ì ID
			n_recommendations: ì¶”ì²œí•  sales ìˆ˜
			min_score: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
		"""
		print(f"ì‚¬ìš©ì {user_id}ì—ê²Œ salesë¥¼ ì¶”ì²œí•˜ëŠ” ì¤‘...")

		profile = self._build_user_profile_vector(user_id)
		if profile is None:
			return []

		# ëª¨ë“  salesì— ëŒ€í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìˆ˜ (í–‰ì´ L2ì •ê·œí™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë‚´ì  == ì½”ì‚¬ì¸)
		scores = self.item_feature_matrix @ profile.reshape(-1, 1)
		scores = np.asarray(scores).ravel()

		candidates = []
		for row_idx, score in enumerate(scores):
			mid = self.sale_index_to_movie_id[row_idx]
			if score >= min_score:
				candidates.append((row_idx, mid, float(score)))

		candidates.sort(key=lambda x: x[2], reverse=True)
		selected = candidates[:n_recommendations]

		result = []
		for row_idx, mid, sc in selected:
			result.append({
				'saleId': self.sale_index_to_id[row_idx],
				'movieId': mid,
				'relevance': round(sc, 4)
			})

		return result

	def evaluate_recommendations(self, user_id, n_recommendations=10):
		"""ì¶”ì²œ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
		recommendations = self.recommend_sales(user_id, n_recommendations=n_recommendations)
		print(f"\n=== ì‚¬ìš©ì {user_id} Sales ê¸°ë°˜ ì¶”ì²œ ê²°ê³¼ ===")
		print(f"ì¶”ì²œ sales ìˆ˜: {len(recommendations)}")
		if recommendations:
			print("\nì¶”ì²œ sales ëª©ë¡:")
			for i, rec in enumerate(recommendations, 1):
				print(f"{i:2d}. saleId={rec['saleId']}, movieId={rec.get('movieId')}")
				print(f"    ìœ ì‚¬ë„: {rec['relevance']}")
				print()
		else:
			print("ì¶”ì²œí•  salesê°€ ì—†ìŠµë‹ˆë‹¤.")
		return recommendations

	def find_best_sales_for_movies(self, movie_recommendations, user_id, top_n=None):
		"""
		Content-based filteringìœ¼ë¡œ ì¶”ì²œë°›ì€ ì˜í™” ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ê° ì˜í™”ì˜ ìµœì  ë¸”ë£¨ë ˆì´(sales) ì°¾ê¸°
		ì‚¬ìš©ìì˜ ê³¼ê±° êµ¬ë§¤/í‰ê°€ íŒ¨í„´ì„ ë¶„ì„í•´ì„œ ë©”íƒ€ë°ì´í„°ê°€ ìœ ì‚¬í•œ ë¸”ë£¨ë ˆì´ë¥¼ ì¶”ì²œ
		
		Args:
			movie_recommendations: ì˜í™” ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ [(movie_id, title, predicted_rating, personalized_score, avg_rating), ...]
			user_id: ì‚¬ìš©ì ID (content-based í”„ë¡œí•„ ìƒì„±ìš©)
			top_n: ìƒìœ„ Nê°œ ë¸”ë£¨ë ˆì´ë§Œ ë°˜í™˜ (Noneì´ë©´ ëª¨ë“  ì˜í™”ì—ì„œ ë¸”ë£¨ë ˆì´ ì°¾ê¸°)
			
		Returns:
			list: ê° ì˜í™”ì˜ ìµœì  sales ì •ë³´ (top_nì´ ì§€ì •ë˜ë©´ ìƒìœ„ Nê°œë§Œ)
		"""
		total_movies = len(movie_recommendations)
		result_count = top_n if top_n else total_movies
		
		print(f"\n=== Content-based Filteringìœ¼ë¡œ ì˜í™” {total_movies}ê°œ â†’ ìƒìœ„ {result_count}ê°œ ë¸”ë£¨ë ˆì´ ì¶”ì²œ ===")
		print(f"ì‚¬ìš©ì ID: {user_id}")
		
		# ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„° ìƒì„± (ì´ë¯¸ í‰ê°€í•œ sales ê¸°ë°˜)
		user_profile = self._build_user_profile_vector(user_id)
		if user_profile is None:
			raise RuntimeError(f"ì‚¬ìš©ì {user_id}ì˜ í”„ë¡œí•„ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìœ ì € í”„ë¡œí•„ì„ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
		
		print(f"âœ… ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„° ìƒì„± ì™„ë£Œ (ì°¨ì›: {len(user_profile)})")
		
		results = []
		movies_with_sales = 0
		
		try:
			# ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´ ì‚¬ìš©ì í”„ë¡œí•„ì„ ì»¬ëŸ¼ ë²¡í„°ë¡œ ì¤€ë¹„
			user_profile_vec = user_profile.reshape(-1, 1)
			for i, movie_rec in enumerate(movie_recommendations, 1):
				movie_id = movie_rec[0]
				movie_title = movie_rec[1]
				# SVD ì˜ˆì¸¡ í‰ì ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ìˆœìˆ˜ ì½˜í…ì¸  ê¸°ë°˜)
				
				# í•´ë‹¹ ì˜í™”ì˜ salesë“¤ì„ content-based filteringìœ¼ë¡œ ì¶”ì²œ
				best_sale_info = self._recommend_sale_for_movie(movie_id, user_profile)
				
				if best_sale_info is None:
					# Salesê°€ ì—†ëŠ” ì˜í™”ëŠ” ë‚®ì€ ì ìˆ˜ë¡œ ì²˜ë¦¬ (ìœ ì‚¬ë„ë§Œ ì‚¬ìš©)
					results.append({
						'movie_id': movie_id,
						'movie_title': movie_title,
						'best_sale': None,
						'similarity_score': -1.0,
						'reason': 'No bluray sales available'
					})
					continue
				
				movies_with_sales += 1
				best_sale, similarity_score, reason = best_sale_info
				
				results.append({
					'movie_id': movie_id,
					'movie_title': movie_title,
					'available_sales': best_sale.get('total_sales', 1) if best_sale else 0,
					'best_sale': best_sale,
					'similarity_score': similarity_score,
					'reason': reason
				})
		
		except Exception as e:
			print(f"ì¶”ì²œ ê³¼ì • ì¤‘ ì˜¤ë¥˜: {e}")
			return []
		
		print(f"âœ… Content-based ì¶”ì²œ ì™„ë£Œ: {movies_with_sales}/{len(movie_recommendations)}ê°œ ì˜í™”ì—ì„œ ë¸”ë£¨ë ˆì´ ë°œê²¬")
		
		# ìƒìœ„ Nê°œë§Œ ë°˜í™˜í•˜ëŠ” ê²½ìš°
		if top_n is not None:
			print(f"ğŸ” ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ {top_n}ê°œ ë¸”ë£¨ë ˆì´ ì„ ë³„ ì¤‘...")
			# ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
			results.sort(key=lambda x: x['similarity_score'], reverse=True)
			results = results[:top_n]
			
			# ì‹¤ì œ ë¸”ë£¨ë ˆì´ê°€ ìˆëŠ” ê²ƒë“¤ë§Œ ì¹´ìš´íŠ¸
			actual_bluray_count = len([r for r in results if r.get('best_sale') is not None])
			print(f"âœ… ìƒìœ„ {top_n}ê°œ ì„ ë³„ ì™„ë£Œ (ë¸”ë£¨ë ˆì´ ìˆìŒ: {actual_bluray_count}ê°œ)")
		
		return results

	def _recommend_sale_for_movie(self, movie_id, user_profile):
		"""
		íŠ¹ì • ì˜í™”ì˜ sales ì¤‘ì—ì„œ ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒì„ content-based filteringìœ¼ë¡œ ì¶”ì²œ
		
		Args:
			movie_id: ì˜í™” ID
			user_profile: ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„°
			
		Returns:
			tuple: (best_sale, similarity_score, reason) ë˜ëŠ” None
		"""
		# í•´ë‹¹ ì˜í™”ì˜ sales ì¸ë±ìŠ¤ë“¤ ì°¾ê¸°
		if movie_id not in self.sale_index_by_movie_id:
			return None
		
		sale_indices = self.sale_index_by_movie_id[movie_id]
		if not sale_indices:
			return None
		
		best_sale = None
		best_score = -1.0
		best_reason = ""

		# ì „ì²´ í›„ë³´ì— ëŒ€í•œ ì ìˆ˜ í•œ ë²ˆì— ê³„ì‚° (í¬ì†Œí–‰ë ¬ ë‚´ì )
		sub_matrix = self.item_feature_matrix[sale_indices]
		scores = (sub_matrix @ user_profile.reshape(-1, 1))
		scores = np.asarray(scores).ravel()
		if scores.size == 0:
			return None

		best_local_idx = int(np.argmax(scores))
		best_score = float(scores[best_local_idx])
		best_row_idx = sale_indices[best_local_idx]
		best_sale_id = self.sale_index_to_id[best_row_idx]

		# ë©”íƒ€ë°ì´í„°ëŠ” ì¸ë©”ëª¨ë¦¬ ë§µì—ì„œ ì¡°íšŒ
		meta = (self.sales_meta_by_id or {}).get(int(best_sale_id))
		if meta is None:
			return None
		best_sale = dict(meta)
		best_sale['total_sales'] = len(sale_indices)
		best_reason = self._get_content_based_reason(best_sale, best_score)
		return (best_sale, best_score, best_reason)


	def _get_content_based_reason(self, sale, similarity_score):
		"""Content-based filtering ì„ íƒ ì´ìœ  ìƒì„±"""
		if not sale:
			return "No sales available"
		
		reasons = []
		
		# í™”ì§ˆ/í˜•ì‹ ì½”ë“œ í•´ì„
		quality = sale.get('quality', '').upper()
		matched_flag = False
		if '4' in quality:
			reasons.append("4K")
			matched_flag = True
		if '3' in quality:
			reasons.append("3D")
			matched_flag = True
		if 'H' in quality:
			reasons.append("HD")
			matched_flag = True
		if 'B' in quality:
			reasons.append("Blu-ray")
			matched_flag = True
		if 'U' in quality:
			reasons.append("ë¯¸í™•ì •")
			matched_flag = True
		if not matched_flag:
			reasons.append("í™”ì§ˆ ì •ë³´ ì—†ìŒ")
		
		# ë¦¬ë¯¸í‹°ë“œ ì—ë””ì…˜
		if sale.get('is_limited_edition'):
			reasons.append("í•œì •íŒ")
		
		# ì§€ì—­
		if sale.get('region_code') == 1:
			reasons.append("êµ­ë‚´íŒ")
		else:
			reasons.append("ìˆ˜ì…íŒ")
		
		# ë¬¸ì¥ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
		items = [r for r in reasons if r]
		if not items:
			return "ê´€ë ¨ íŠ¹ì„±ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤."
		return f"ë‹¹ì‹ ì´ ì„ í˜¸í•˜ëŠ” {', '.join(items)} íŠ¹ì„±ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤."
